# ğŸ¡ **House Prices â€” Advanced Regression Techniques**
### ğŸ¯ *End-to-End Machine Learning Pipeline (Kaggle Competition)*

This project is an end-to-end regression pipeline built using the **Kaggle House Prices: Advanced Regression Techniques** dataset.  
The goal is to develop a machine learning system that predicts the **SalePrice** of a home as accurately as possible based on its physical and qualitative features.

---

## ğŸ“‚ **Dataset Story**

This project uses the well-known **Ames Housing Dataset**, consisting of:

- **79 explanatory variables**
- **1 target variable (SalePrice)**
- **1,460 observations in the training set**
- **1,459 observations in the test set**

The dataset includes a wide range of informationâ€”physical attributes, renovation status, material quality, location, and overall house condition.

---

## ğŸ§­ **Roadmap of the Project**

### **1ï¸âƒ£ Exploratory Data Analysis (EDA)**  
- Numerical & categorical feature distributions  
- Correlation analysis with SalePrice  
- Missing value inspection  
- Pairplots, heatmaps, boxplots  

### **2ï¸âƒ£ Feature Engineering**  
- Missing value imputation (median, mode, custom flags)  
- Rare category handling  
- Outlier detection using IQR  
- New feature creation:
  - `TotalSF`
  - Age-related features (HouseAge, RemodelAge)
  - Quality Ã— Area interactions  

### **3ï¸âƒ£ Preprocessing**  
- Label Encoding & One-Hot Encoding  
- Scaling (StandardScaler)  
- Train/Test split  

### **4ï¸âƒ£ Modeling**  
Models trained and evaluated:

- **LightGBM**
- **Random Forest**
- **XGBoost**
- **GradientBoostingRegressor**
- **Linear Models** (Ridge / Lasso / ElasticNet)

### **5ï¸âƒ£ Hyperparameter Optimization**  
- GridSearchCV  
- RandomizedSearchCV  
- 5-Fold Cross-Validation  
- RMSE-based scoring  

### **6ï¸âƒ£ Model Evaluation**  
- Train/Test RMSE  
- Cross-validation RMSE  
- Residual plots  
- Feature importance visualizations  

---

## âš ï¸ **About Warnings**

During execution, you may encounter warnings such as:



This typically occurs when Seaborn attempts to compute a mean on an empty subset of data (e.g., when a category has no observations after filtering).

Additional warnings suppressed for cleaner output:

warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter("ignore", category=ConvergenceWarning)


These do not affect model performance.

ğŸ§  Technologies & Skills Used

Python

Pandas, NumPy

Seaborn, Matplotlib

Scikit-Learn

LightGBM, XGBoost

Feature Engineering

Hyperparameter Tuning

Cross-Validation

Regression Metrics: RMSE, MAE, RÂ²

ğŸ—‚ï¸ Project Structure
house-price-regression/
â”‚â”€â”€ README.md
â”‚â”€â”€ notebook.ipynb
â”‚â”€â”€ data/
â”‚     â”œâ”€â”€ train.csv
â”‚     â”œâ”€â”€ test.csv
â”‚â”€â”€ models/
â”‚â”€â”€ utils/


ğŸ“Š Model Performance

The best performance was typically achieved using LightGBM with optimized hyperparameters.

CV RMSE: ~0.12 â€“ 0.13

Test RMSE: Competitive Kaggle leaderboard score
(Exact value varies depending on feature engineering & tuning.)

ğŸš€ How to Run the Project
git clone https://github.com/<username>/house-price-regression.git
cd house-price-regression
pip install -r requirements.txt
jupyter notebook

